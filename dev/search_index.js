var documenterSearchIndex = {"docs":
[{"location":"nls/#nls-section","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"","category":"section"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The nonlinear least squares (NLS) optimization problem is a specific case where the objective function is a sum of squares.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  f(x)=tfrac12F(x)^2_2 \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Although the problem can be solved using only  f, knowing  F independently allows the development of more efficient methods.","category":"page"},{"location":"nls/#Model-and-solve-NLS","page":"Nonlinear Least Squares","title":"Model and solve NLS","text":"","category":"section"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"In this tutorial, we consider the following equality-constrained problem","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  f(x)=tfrac12(10 * (x2 - x1^2))^2 + tfrac12(x1 - 1)^2 \n 1 leq x1 * x2 leq 1\nendaligned","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"where 1 leq x1 x2 leq 1 implies that x1 x2 = 1.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"There are two important challenges in solving an optimization problem: (i) model the problem, and (ii) solve the problem with an appropriate solve.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Let's see two ways to model this problem exploiting the knowledge of the structure of the problem.","category":"page"},{"location":"nls/#NLS-using-automatic-differentiation","page":"Nonlinear Least Squares","title":"NLS using automatic differentiation","text":"","category":"section"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Using the package ADNLPModels.jl, the problem can be model as an ADNLSModel which will use automatic-differentiation to compute the derivatives.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ADNLPModels, JSOSuite\nF = x -> [10 * (x[2] - x[1]^2); x[1] - 1]\nnres = 2 # size of F(x)\nx0 = [-1.2; 1.0]\nc = x -> [x[1] * x[2]]\nl = [1.]\nnls = ADNLSModel(F, x0, nres, c, l, l, name=\"AD-Rosenbrock\")","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Note that the length of the residual function is given explictly to avoid any superfluous evaluation of this (potentially very large) function.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"stats = solve(nls)","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"JSOSuite.jl uses by default automatic differentiation, so the following code would be equivalent:","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"stats = solve(F, x0, nres, c, l, l)","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"By default, JSOSuite.solve will use a solver tailored for nonlineat least squares problem. Nevertheless, it is also possible to specify the solver to be used.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"stats = solve(\"IPOPT\", F, x0, nres, c, l, l)","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"We refer to the documentation of ADNLPModels.jl for more details on the AD system use and how to modify it.","category":"page"},{"location":"nls/#NLS-using-JuMP","page":"Nonlinear Least Squares","title":"NLS using JuMP","text":"","category":"section"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The package NLPModelsJuMP.jl exports a constructor, MathOptNLSModel, to build an AbstractNLSModel using JuMP.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using JuMP, JSOSuite, NLPModelsJuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2], start=x0[i])\n@NLexpression(model, F1, x[1] - 1)\n@NLexpression(model, F2, 10 * (x[2] - x[1]^2))\n@NLconstraint(model, x[1] * x[2] == 1)\n\nnls = MathOptNLSModel(model, [F1, F2], name=\"Ju-Rosenbrock\")\nstats = solve(nls)","category":"page"},{"location":"nls/#Find-a-feasible-point-of-an-optimization-problem-or-solve-a-nonlinear-system","page":"Nonlinear Least Squares","title":"Find a feasible point of an optimization problem or solve a nonlinear system","text":"","category":"section"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"We show here how to find the feasible point of a given model. ","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  tfrac12s^2_2 \n 0 leq s - c(x) leq 0\n ell leq x leq u\nendaligned","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"This formulation can also be used to solve a set of nonlinear equations. Finding a feasible point of an optimization problem is useful to find the problem is feasible and it is a good practice to find an initial guess.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ADNLPModels, JSOSuite\n\nf = x -> sum(x.^2)\nx0 = ones(3)\nc = x -> [x[1]]\nb = zeros(1)\nnlp = ADNLPModel(f, x0, c, b, b)\nstats = feasible_point(nlp)","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Using the function cons from the NLPModel API, we can verify that the obtained solution is feasible.","category":"page"},{"location":"nls/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using NLPModels\n\ncons(nlp, stats.solution) # is close to zero.","category":"page"},{"location":"speed-up/#speed-up","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"The following contains a list of tips to speed up the solver selection and usage.","category":"page"},{"location":"speed-up/#Derivatives","page":"Speed up Solvers Tips","title":"Derivatives","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"The solvers available in JSOSuite.jl are all using first and sometines second-order derivatives. There are mainly three categories:","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"1st order methods use only gradient information;\n1st order quasi-Newton methods require only gradient information, and uses it to build an approximation of the Hessian;\n2nd order methods: Those are using gradients and Hessian information.\n2nd order methods matrix-free: Those are solvers using Hessian information, but without ever forming the matrix, so only matrix-vector products are computed.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"The latter is usually a good tradeoff for very large problems.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nstats = solve(f, x0, verbose = 0, highest_derivative_available = 1)\nstats","category":"page"},{"location":"speed-up/#Find-a-better-initial-guess","page":"Speed up Solvers Tips","title":"Find a better initial guess","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"The majority of derivative-based solvers are local methods whose performance are dependent of the initial guess.  This usually relies on specific knowledge of the problem.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"The function feasible_point computes a point satisfying the constraints of the problem that can be used as an initial guess.  An alternative is to solve a simpler version of the problem and reuse the solution as an initial guess for the more complex one.","category":"page"},{"location":"speed-up/#Use-the-structure-of-the-problem","page":"Speed up Solvers Tips","title":"Use the structure of the problem","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"If the problem has linear constraints, then it is efficient to specify it at the modeling stage to avoid having them treated like nonlinear ones. Some of the solvers will also exploit this information.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"Similarly, quadratic objective or least squares problems have tailored modeling tools and solvers.","category":"page"},{"location":"speed-up/#Change-the-parameters-of-the-solver","page":"Speed up Solvers Tips","title":"Change the parameters of the solver","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"Once a solver has been chosen it is also possible to play with the key parameters. Find below a list of the available solvers and parameters.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"Note that all solvers presented here have been carefully optimized. All have different strengths. Trying another solver on the same problem sometimes provide a different solution.","category":"page"},{"location":"speed-up/#Unconstrained/Bound-constrained","page":"Speed up Solvers Tips","title":"Unconstrained/Bound-constrained","text":"","category":"section"},{"location":"speed-up/#LBFGS","page":"Speed up Solvers Tips","title":"LBFGS","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"mem::Int = 5: memory parameter of the lbfgs algorithm;\nτ₁::T = T(0.9999): slope factor in the Wolfe condition when performing the line search;\nbk_max:: Int = 25: maximum number of backtracks when performing the line search.","category":"page"},{"location":"speed-up/#TRON","page":"Speed up Solvers Tips","title":"TRON","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"μ₀::T = T(1e-2): algorithm parameter in (0, 0.5);\nμ₁::T = one(T): algorithm parameter in (0, +∞);\nσ::T = T(10): algorithm parameter in (1, +∞);\nmax_cgiter::Int = 50: subproblem's iteration limit;\ncgtol::T = T(0.1): subproblem tolerance.","category":"page"},{"location":"speed-up/#TRUNK","page":"Speed up Solvers Tips","title":"TRUNK","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"TODO","category":"page"},{"location":"speed-up/#R2","page":"Speed up Solvers Tips","title":"R2","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"η1 = eps(T)^(1/4), η2 = T(0.95): step acceptance parameters;\nγ1 = T(1/2), γ2 = 1/γ1: regularization update parameters;\nσmin = eps(T): step parameter for R2 algorithm;\nβ = T(0) ∈ [0,1] is the constant in the momentum term. If β == 0, R2 does not use momentum.","category":"page"},{"location":"speed-up/#Constrained","page":"Speed up Solvers Tips","title":"Constrained","text":"","category":"section"},{"location":"speed-up/#Percival","page":"Speed up Solvers Tips","title":"Percival","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"μ::Real = T(10.0): Starting value of the penalty parameter.","category":"page"},{"location":"speed-up/#CaNNOLeS","page":"Speed up Solvers Tips","title":"CaNNOLeS","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"linsolve::Symbol = :ma57: solver to compute LDLt factorization. Available methods are: :ma57, :ldlfactorizations;\nmethod::Symbol = :Newton: available methods :Newton, :LM, :Newton_noFHess, and :Newton_vanishing;","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"See CaNNOLeS.jl tutorial.","category":"page"},{"location":"speed-up/#DCISolver","page":"Speed up Solvers Tips","title":"DCISolver","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"linear_solver = :ldlfact: Solver for the factorization. options: :ma57 if HSL.jl available.","category":"page"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"See fine-tuneDCI.","category":"page"},{"location":"speed-up/#RipQP","page":"Speed up Solvers Tips","title":"RipQP","text":"","category":"section"},{"location":"speed-up/","page":"Speed up Solvers Tips","title":"Speed up Solvers Tips","text":"TODO","category":"page"},{"location":"qp/#qp","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"","category":"section"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"The quadratic model with linear constraints is another specific case where the objective function is quadratic and the constraints are linear.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"beginaligned\nmin quad   x^TQx + c^Tx + c_0  \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"This problem is convex whenever the matrix Q is positive semi-definite. A key aspect here is the modeling of the matrices Q and A.  The main data structure available in Julia are: LinearAlgebra.Matrix, SparseArrays.sparse, SparseMatricesCOO.sparse, LinearOperators.LinearOperator.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"In JuliaSmoothOptimizers, the package QuadraticModels.jl can be used to access the NLPModel API for such instance.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"The function solve with the following sets of arguments will automatically build a QuadraticModel and choose the adequate solver.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"  stats = solve(c, H, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = solve(c, H, lvar, uvar, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = solve(c, H, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = solve(c, H, lvar, uvar, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)","category":"page"},{"location":"qp/#Example","page":"Quadratic models with linear constraints","title":"Example","text":"","category":"section"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"  using SparseArrays\n  n = 50\n  c = zeros(n)\n  H = spdiagm(0 => 1.0:n)\n  H[n, 1] = 1.0\n  A = ones(1, n)\n  lcon = [1.0]\n  ucon = [1.0]","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"The quadratic model can then be solved using solve.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"using JSOSuite\n  stats = solve(c, H, A, lcon, ucon, name = \"eqconqp_QP\")","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"This is equivalent to building a QuadraticModel and then solve.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"using QuadraticModels, JSOSuite\n  qp_model = QuadraticModel(c, H, A, lcon, ucon, name = \"eqconqp_QP\")\n  stats = solve(qp_model)","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"As usual, it is also possible to select manually the solver to be used.","category":"page"},{"location":"qp/","page":"Quadratic models with linear constraints","title":"Quadratic models with linear constraints","text":"  stats = solve(\"RipQP\", c, H, A, lcon, ucon, name = \"eqconqp_QP\")","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Pages = [\"reference.md\"]","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"​","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [JSOSuite]","category":"page"},{"location":"reference/#JSOSuite.solvers","page":"Reference","title":"JSOSuite.solvers","text":"solvers\n\nDataFrame with the JSO-compliant solvers and their properties.\n\nFor each solver, the following are available:\n\nname::String: name of the solver;\nname_solver::Symbol: name of the solver structure for in-place solve, :not_implemented if not implemented;\nname_pkg::String: name of the package implementing this solver or its NLPModel wrapper;\nsolve_function::Symbol: name of the function;\nis_available::Bool: true if the solver is available;\nbounds::Bool: true if the solver can handle bound constraints;\nequalities::Bool: true if the solver can handle equality constraints;\ninequalities::Bool: true if the solver can handle inequality constraints;\nspecialized_nls::Bool: true if the solver has a specialized variant for nonlinear least squares;\ncan_solve_nlp::Bool: true if the solver can solve general problems. Some may only solve nonlinear least squares;\nnonlinear_obj::Bool: true if the solver can handle nonlinear objective;\nnonlinear_con::Bool: true if the solver can handle nonlinear constraints;\ndouble_precision_only::Bool: true if the solver only handles double precision (Float64);\nhighest_derivative::Int: order of the highest derivative used by the algorithm.\n\n\n\n\n\n","category":"constant"},{"location":"reference/#JSOSuite.feasible_point","page":"Reference","title":"JSOSuite.feasible_point","text":"stats = feasible_point(nlp::Union{AbstractNLPModel, JuMP.Model}; kwargs...)\nstats = feasible_point(nlp::Union{AbstractNLPModel, JuMP.Model}, solver_name::Symbol; kwargs...)\n\nCompute a feasible point of the optimization problem nlp. The signature is the same as the function solve.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl, where the status, solution, primal_residual, iter and time are filled-in.\n\nusing ADNLPModels, JSOSuite\nc(x) = [10 * (x[2] - x[1]^2); x[1] - 1]\nb = zeros(2)\nnlp = ADNLPModel(x -> 0.0, [-1.2; 1.0], c, b, b)\nstats = feasible_point(nlp, verbose = 0)\nstats\n\n\n\n\n\n","category":"function"},{"location":"reference/#JSOSuite.generic","page":"Reference","title":"JSOSuite.generic","text":"Checker whether solvers are Generic only\n\n\n\n\n\n","category":"function"},{"location":"reference/#JSOSuite.select_solvers-Union{Tuple{NLPModels.AbstractNLPModel{T, S}}, Tuple{S}, Tuple{T}, Tuple{NLPModels.AbstractNLPModel{T, S}, Any}, Tuple{NLPModels.AbstractNLPModel{T, S}, Any, Integer}} where {T, S}","page":"Reference","title":"JSOSuite.select_solvers","text":"select_solvers(nlp::AbstractNLPModel, verbose = 1, highest_derivative_available::Integer = 2)\n\nNarrow the list of solvers to solve nlp problem using highest_derivative_available.\n\nThis function checks whether the model has:\n\nlinear or nonlinear constraints;\nunconstrained, bound constraints, equality constraints, inequality constraints;\nnonlinear or quadratic objective.\n\nA linear or quadratic objective is detected if the type of nlp is a QuadraticModel or an LLSModel. The selection between a general optimization problem and a nonlinear least squares is done in solve.\n\nIf no solvers were selected, consider setting verbose to true to see what went wrong.\n\nOutput\n\nselected_solvers::DataFrame: A subset of solvers adapted to the problem nlp.\n\nSee also solve.\n\nExamples\n\nusing ADNLPModels, JSOSuite\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nselected_solvers = JSOSuite.select_solvers(nlp)\nprint(selected_solvers[!, :name])\n\nusing ADNLPModels, JSOSuite\nnlp = ADNLSModel(x -> [10 * (x[2] - x[1]^2), (x[1] - 1)], [-1.2; 1.0], 2)\nselected_solvers = JSOSuite.select_solvers(nlp)\nprint(selected_solvers[!, :name])\n\n\n\n\n\n","category":"method"},{"location":"reference/#JSOSuite.solve","page":"Reference","title":"JSOSuite.solve","text":"stats = solve(nlp::Union{AbstractNLPModel, JuMP.Model}; kwargs...)\n\nCompute a local minimum of the optimization problem nlp.\n\nstats = solve(f::Function, x0::AbstractVector, args...; kwargs...)\nstats = solve(F::Function, x0::AbstractVector, nequ::Integer, args...; kwargs...)\n\nDefine an NLPModel using ADNLPModel.\n\nstats = solve(c, H, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = solve(c, H, lvar, uvar, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = solve(c, H, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = solve(c, H, lvar, uvar, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\n\nDefine a QuadraticModel using QuadraticModel.\n\nThe solver can be chosen as follows.\n\nstats = solve(solver_name::String, args...; kwargs...)\n\nJuMP.Model are converted in NLPModels via NLPModelsJuMP.jl.\n\nIf your optimization problem has a quadratic or linear objective and linear constraints consider using QuadraticModels.jl or LLSModels.jl for the model definition.\n\nKeyword Arguments\n\nAll the keyword arguments are passed to the selected solver. Keywords available for all the solvers are given below:\n\natol: absolute tolerance;\nrtol: relative tolerance;\nmax_time: maximum number of seconds;\nmax_iter: maximum number of iterations;\nmax_eval: maximum number of cons + obj evaluations;\ncallback = (args...) -> nothing: callback called at each iteration;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nThe following are specific to nonlinear least squares:\n\nFatol::T = √eps(T): absolute tolerance on the residual;\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\n\nFurther possible options are documented in each solver's documentation.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nExamples\n\nusing JSOSuite\nstats = solve(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0], verbose = 0)\nstats\n\nThe list of available solver can be obtained using JSOSuite.solvers[!, :name] or see select_solvers.\n\nusing JSOSuite\nstats = solve(\"DCISolver\", x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0], verbose = 0)\nstats\n\nusing JSOSuite\n# We solve here a quadratic problem with bound-constraints\nc = [1.0; 1.0]\nH = [-2.0 0.0; 3.0 4.0]\nuvar = [1.0; 1.0]\nlvar = [0.0; 0.0]\nx0 = [0.5; 0.5]\nstats = solve(\"TRON\", c, H, lvar, uvar, x0 = x0, name = \"bndqp_QP\", verbose = 0)\nstats\n\n\n\n\n\n","category":"function"},{"location":"reference/#SolverBenchmark.bmark_solvers","page":"Reference","title":"SolverBenchmark.bmark_solvers","text":"bmark_solvers(problems, solver_names::Vector{String}; kwargs...)\nbmark_solvers(problems, solver_names::Vector{String}, solvers::Dict{Symbol, Function}; kwargs...)\n\nWrapper to the function SolverBenchmark.bmark_solvers.\n\nArguments\n\nproblems: The set of problems to pass to the solver, as an iterable ofAbstractNLPModel;\nsolver_names::Vector{String}: The names of the benchmarked solvers. They should be valid JSOSuite names, see JSOSuite.solvers.name for a list;\nsolvers::solvers::Dict{Symbol, Function}: A dictionary of additional solvers to benchmark.\n\nOutput\n\nA Dict{Symbol, DataFrame} of statistics.\n\nKeyword Arguments\n\nThe following keywords available are passed to the JSOSuite solvers:\n\natol: absolute tolerance;\nrtol: relative tolerance;\nmax_time: maximum number of seconds;\nmax_eval: maximum number of cons + obj evaluations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nAll the remaining keyword arguments are passed to the function SolverBenchmark.bmark_solvers.\n\nExamples\n\nusing ADNLPModels, JSOSuite\nnlps = (\n  ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n  ADNLPModel(x -> 4 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n)\nnames = [\"LBFGS\", \"TRON\"] # see `JSOSuite.solvers.name` for the complete list\nstats = bmark_solvers(nlps, names, atol = 1e-3, verbose = 0, colstats = [:name, :nvar, :ncon, :status])\nkeys(stats)\n\nThe second example shows how to add you own solver to the benchmark.\n\nusing ADNLPModels, JSOSolvers, JSOSuite, Logging\nnlps = (\n  ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n  ADNLPModel(x -> 4 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n)\nnames = [\"LBFGS\", \"TRON\"] # see `JSOSuite.solvers.name` for the complete list\nother_solvers = Dict{Symbol, Function}(\n  :test => nlp -> lbfgs(nlp; mem = 2, atol = 1e-3, verbose = 0),\n)\nstats = bmark_solvers(nlps, names, other_solvers, atol = 1e-3, verbose = 0, colstats = [:name, :nvar, :ncon, :status])\nkeys(stats)\n\n\n\n\n\n","category":"function"},{"location":"benchmark/#Benchmarking-solvers","page":"Benchmarking","title":"Benchmarking solvers","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Benchmarking is very important when researching new algorithms or selecting the most approriate ones.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"The package SolverBenchmark exports the function bmark_solvers that runs a set of solvers on a set of problems. JSOSuite.jl specialize this function, see bmark_solvers.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"The JuliaSmoothOptimizers organization contains several packages of test problems ready to use for benchmarking. The main ones are","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"OptimizationProblems.jl: This package provides a collection of optimization problems in JuMP and ADNLPModels syntax;\nCUTEst.jl;\nNLSProblems.jl.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"In this tutorial, we will use OptimizationProblems.jl with automatic differentiation.","category":"page"},{"location":"benchmark/#Benchmark-with-OptimizationProblems.jl","page":"Benchmarking","title":"Benchmark with OptimizationProblems.jl","text":"","category":"section"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"using ADNLPModels, OptimizationProblems, JSOSuite","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"The package OptimizationProblems.jl provides a meta containing all the problem information. It is possible to then select the problems without evaluating them first.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"selected_meta = OptimizationProblems.meta\nselected_meta = selected_meta[(selected_meta.nvar .< 200), :] # choose problem with <200 variables.\nselected_meta = selected_meta[.!selected_meta.has_bounds .&& (selected_meta.ncon .== 0), :]; # unconstrained problems\nlist = selected_meta[!, :name]","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Then, we generate the list of problems using ADNLPModel.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"ad_problems = [\n  OptimizationProblems.ADNLPProblems.eval(Meta.parse(problem))() for problem ∈ list\n]\nlength(ad_problems) # return the number of problems","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"We now want to select appropriate solvers using the JSOSuite.solvers.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"selected_solvers = JSOSuite.solvers\n# solvers can solve general `nlp` as some are specific to variants (NLS, ...)\nselected_solvers = selected_solvers[selected_solvers.can_solve_nlp, :]\nselected_solvers[selected_solvers.is_available, :] # solvers available","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"For the purpose of this example, we will consider 3 solvers.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"select = [\"IPOPT\", \"TRUNK\", \"LBFGS\"]","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Once the problems and solvers are chosen, the function bmark_solvers runs the benchmark. ","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"stats = bmark_solvers(ad_problems, select, atol = 1e-3, max_time = 10.0, verbose = 0)","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Finally, the result can be processed. For instance, we show here performance profiles comparing the elapsed time and the number of evaluations.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"first_order(df) = df.status .== :first_order\nunbounded(df) = df.status .== :unbounded\nsolved(df) = first_order(df) .| unbounded(df)\ncostnames = [\"time\", \"obj + grad + hess\"]\ncosts = [\n  df -> .!solved(df) .* Inf .+ df.elapsed_time,\n  df -> .!solved(df) .* Inf .+ df.neval_obj .+ df.neval_grad,\n]\n\nusing Plots, SolverBenchmark\ngr()\n\nprofile_solvers(stats, costs, costnames)","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"Note that there are fundamental differences between these solvers as highlighted in the following.","category":"page"},{"location":"benchmark/","page":"Benchmarking","title":"Benchmarking","text":"for solver in [\"IPOPT\", \"TRUNK\", \"LBFGS\"]\n  println(\"$solver evaluations:\")\n  a, b, c, d = eachcol(\n    stats[Symbol(solver)][!, [:neval_obj, :neval_grad, :neval_hess, :neval_hprod]]\n  )\n  println(\n    \"neval_obj: $(sum(a)),  neval_grad: $(sum(b)), neval_hess: $(sum(c)), neval_hprod: $(sum(d)).\"\n  )\nend","category":"page"},{"location":"#JSOSuite.jl","page":"Home","title":"JSOSuite.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JSOSuite is a unique solution to access all the solvers available in the JuliaSmoothOptimizers organization.","category":"page"},{"location":"","page":"Home","title":"Home","text":"All these solvers rely on the NLPModel API from NLPModels.jl for general nonlinear optimization problems of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package JSOSuite exports a function solve: ","category":"page"},{"location":"","page":"Home","title":"Home","text":"output = solve(args...; kwargs...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The arguments are used to define the problem, see Tutorial.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is also possible to define an NLPModel or a JuMP model representing the problem, and then call solve:","category":"page"},{"location":"","page":"Home","title":"Home","text":"output = solve(nlpmodel; kwargs...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The NLPModel API is a general consistent API for solvers to interact with models by providing flexible data types to represent the objective and constraint functions to evaluate their derivatives, and to provide essentially any information that a solver might request from a model. JuliaSmoothOrganization's website or NLPModels.jl's documentation provide more tutorials on this topic.","category":"page"},{"location":"#NLPModel","page":"Home","title":"NLPModel","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JuliaSmoothOptimizers' compliant solvers accept any model compatible with the NLPModel API. See the Tutorial section for examples.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depending on the origin of the problem several modeling tools are available. The following generic modeling tools are accepted:","category":"page"},{"location":"","page":"Home","title":"Home","text":"JuMP models are internally made compatible with NLPModel via NLPModelsJuMP.jl.\nAmpl models stored in a .nl file can AmplModel(\"name_of_file.nl\") using AmplNLReader.jl.\nQPSReader.jl reads linear problems in MPS format and quadratic problems in QPS format.\nModels using automatic differentiation can be generated using ADNLPModels.jl.\nModels with manually input derivatives can be defined using ManualNLPModels.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is also possible to define your NLPModel variant. Several examples are available within JuliaSmoothOptimizers' umbrella:","category":"page"},{"location":"","page":"Home","title":"Home","text":"KnetNLPModels.jl: An NLPModels Interface to Knet.\nPDENLPModels.jl: A NLPModel API for optimization problems with PDE-constraints.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A nonlinear least squares problem is a special case with the objective function defined as  f(x) = tfrac12F(x)^2_2. Although the problem can be solved using only  f, knowing  F independently allows the development of more efficient methods. See the Nonlinear Least Squares for special treatment of these problems.","category":"page"},{"location":"#Output","page":"Home","title":"Output","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The value returned is a GenericExecutionStats, which is a structure containing the available information at the end of the execution, such as a solver status, the objective function value, the norm of the residuals, the elapsed time, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It contains the following fields:","category":"page"},{"location":"","page":"Home","title":"Home","text":"status: Indicates the output of the solver. Use show_statuses() for the full list;\nsolution: The final approximation returned by the solver (default: an uninitialized vector like nlp.meta.x0);\nobjective: The objective value at solution (default: Inf);\ndual_feas: The dual feasibility norm at solution (default: Inf);\nprimal_feas: The primal feasibility norm at solution (default: 0.0 if unconstrained, Inf otherwise);\nmultipliers: The Lagrange multipliers wrt to the constraints (default: an uninitialized vector like nlp.meta.y0);\nmultipliers_L: The Lagrange multipliers wrt to the lower bounds on the variables (default: an uninitialized vector like nlp.meta.x0 if there are bounds, or a zero-length vector if not);\nmultipliers_U: The Lagrange multipliers wrt to the upper bounds on the variables (default: an uninitialized vector like nlp.meta.x0 if there are bounds, or a zero-length vector if not);\niter: The number of iterations computed by the solver (default: -1);\nelapsed_time: The elapsed time computed by the solver (default: Inf);\nsolver_specific::Dict{Symbol,Any}: A solver specific dictionary.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The list of statuses is available via the function SolverCore.show_statuses:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SolverCore\nshow_statuses()","category":"page"},{"location":"#Keyword-Arguments","page":"Home","title":"Keyword Arguments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All the keyword arguments are passed to the selected solver. Keywords available for all the solvers are given below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"atol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nmax_time::Float64 = 300.0: maximum number of seconds;\nmax_iter::Int = typemax(Int): maximum number of iterations;\nmax_eval::Int = 10 000: maximum number of constraint and objective functions evaluations;\ncallback = (args...) -> nothing: callback called at each iteration;\nverbose::Int = 0: if > 0, display iteration details for every verbose iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following are specific to nonlinear least squares:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fatol::T = √eps(T): absolute tolerance on the residual;\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Further possible options are documented in each solver's documentation.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"] add JSOSuite","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Bug-reports-and-discussions","page":"Home","title":"Bug reports and discussions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"","page":"Home","title":"Home","text":"If you want to ask a question not suited for a bug report, feel free to start a discussion here. This forum is for general discussion about this repository and the JuliaSmoothOptimizers, so questions about any of our packages are welcome.","category":"page"},{"location":"tutorial/#tutorial-section","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In this tutorial, we provide examples of usage of the solve function exported by JSOSuite.jl.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"There are two important challenges in solving an optimization problem: (i) model the problem, and (ii) solve the problem with an appropriate solver.","category":"page"},{"location":"tutorial/#Modeling","page":"Tutorial","title":"Modeling","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All these solvers rely on the NLPModel API from NLPModels.jl for general nonlinear optimization problems of the form","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The function solve accepts as an argument any model nlp subtype of AbstractNLPModel.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"output = solve(nlpmodel::AbstractNLPModel; kwargs...)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"In the rest of this section, we focus on examples using generic modeling tools.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"It is generally of great interest if available to use a modeling that handles the structure of the problem, see Nonlinear Least Squares for an example with nonlinear least squares.","category":"page"},{"location":"tutorial/#JuMP-Model","page":"Tutorial","title":"JuMP Model","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JuMP, JSOSuite\nmodel = Model()\n@variable(model, x)\n@variable(model, y)\n@NLobjective(model, Min, (1 - x)^2 + 100 * (y - x^2)^2)\n\nsolve(model)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We refer to JuMP tutorial.","category":"page"},{"location":"tutorial/#NLPModel-with-Automatic-Differentiation","page":"Tutorial","title":"NLPModel with Automatic Differentiation","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"We refer to ADNLPModel for the description of the different constructors.","category":"page"},{"location":"tutorial/#Unconstrained","page":"Tutorial","title":"Unconstrained","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nstats = solve(f, x0, verbose = 0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = solve(nlp)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"One of the main advantages of this constructor is the possibility to run computations in different arithmetics. ","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = Float32[-1.2; 1.0]\nstats = solve(f, x0, verbose = 0)","category":"page"},{"location":"tutorial/#Bound-constrained","page":"Tutorial","title":"Bound-constrained","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nlvar, uvar = 2 * ones(2), 4 * ones(2)\nstats = solve(f, x0, lvar, uvar, verbose = 0)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = solve(nlp)","category":"page"},{"location":"tutorial/#Nonlinear-constrained","page":"Tutorial","title":"Nonlinear constrained","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nc = x -> [x[1]]\nl = ones(1)\nstats = solve(f, x0, c, l, l, verbose = 0)","category":"page"},{"location":"tutorial/#Linearly-constrained","page":"Tutorial","title":"Linearly constrained","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite, SparseArrays\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nA = sparse([\n    1.0 0.0;\n    2.0 3.0\n])\nl = ones(2)\nstats = solve(f, x0, A, l, l, verbose = 0)","category":"page"},{"location":"tutorial/#All-constraints","page":"Tutorial","title":"All constraints","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite, SparseArrays\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nA = sparse([2.0 3.0])\nc = x -> [x[1]]\nl = ones(2)\nstats = solve(f, x0, A, c, l, l, verbose = 0)","category":"page"},{"location":"tutorial/#Solving","page":"Tutorial","title":"Solving","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Internally, the solve function selects solvers according to the problem's property and JSO-compliant solvers available.","category":"page"},{"location":"tutorial/#Available-solvers","page":"Tutorial","title":"Available solvers","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All the information used by the handled solvers is available in the following DataFrame:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nJSOSuite.solvers","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Required information can be extracted by simple DataFrame manipulations. For instance, the list of solvers handled by this package","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"JSOSuite.solvers.name","category":"page"},{"location":"tutorial/#Select-solvers","page":"Tutorial","title":"Select solvers","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"The function JSOSuite.select_solvers returns a list of compatible solvers.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nJSOSuite.select_solvers(nlp)","category":"page"},{"location":"tutorial/#Fine-tune-solve-call","page":"Tutorial","title":"Fine-tune solve call","text":"","category":"section"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"All the keyword arguments are passed to the solver. Keywords available for all the solvers are given below:","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"atol: absolute tolerance;\nrtol: relative tolerance;\nmax_time: maximum number of seconds;\nmax_eval: maximum number of cons + obj evaluations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = solve(nlp, atol = 1e-5, rtol = 1e-7, max_time = 10.0, max_eval = 10000, verbose = 1)","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"Further possible options are documented in each solver's documentation. For instance, we can update the mem parameter of LBFGS.","category":"page"},{"location":"tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = solve(\"LBFGS\", nlp, mem = 10, verbose = 1)","category":"page"}]
}
