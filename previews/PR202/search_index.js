var documenterSearchIndex = {"docs":
[{"location":"60-speed-up/#speed-up","page":"Speed Up","title":"Speed up Solvers Tips","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"The following contains a list of tips to speed up the solver selection and usage.","category":"page"},{"location":"60-speed-up/#Derivatives","page":"Speed Up","title":"Derivatives","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"The optimizers available in JSOSuite.jl are all using first and sometines second-order derivatives. There are mainly three categories:","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"1st order methods use only gradient information;\n1st order quasi-Newton methods require only gradient information, and uses it to build an approximation of the Hessian;\n2nd order methods: Those are using gradients and Hessian information.\n2nd order methods matrix-free: Those are optimizers using Hessian information, but without ever forming the matrix, so only matrix-vector products are computed.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"The latter is usually a good tradeoff for very large problems.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nstats = minimize(f, x0, verbose = 0, highest_derivative_available = 1)\nstats","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"This classification is straightforwardly extended to handling constraints with the Jacobian matrix explicitly of via matrix-vector products.","category":"page"},{"location":"60-speed-up/#Find-a-better-initial-guess","page":"Speed Up","title":"Find a better initial guess","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"The majority of derivative-based optimizers are local methods whose performance are dependent of the initial guess. This usually relies on specific knowledge of the problem.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"The function feasible_point computes a point satisfying the constraints of the problem that can be used as an initial guess. An alternative is to solve a simpler version of the problem and reuse the solution as an initial guess for the more complex one.","category":"page"},{"location":"60-speed-up/#Use-the-structure-of-the-problem","page":"Speed Up","title":"Use the structure of the problem","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"If the problem has linear constraints, then it is efficient to specify it at the modeling stage to avoid having them treated like nonlinear ones. Some of the optimizers will also exploit this information.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"Similarly, quadratic objective or least squares problems have tailored modeling tools and optimizers.","category":"page"},{"location":"60-speed-up/#Change-the-parameters-of-the-solver","page":"Speed Up","title":"Change the parameters of the solver","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"Once a solver has been chosen it is also possible to play with the key parameters. Find below a list of the available optimizers and parameters.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"Note that all optimizers presented here have been carefully optimized. All have different strengths. Trying another solver on the same problem sometimes provide a different solution.","category":"page"},{"location":"60-speed-up/#Unconstrained","page":"Speed Up","title":"Unconstrained","text":"","category":"section"},{"location":"60-speed-up/#LBFGS-(1st-order)","page":"Speed Up","title":"LBFGS (1st order)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"mem::Int = 5: memory parameter of the lbfgs algorithm;\nτ₁::T = T(0.9999): slope factor in the Wolfe condition when performing the line search;\nbk_max:: Int = 25: maximum number of backtracks when performing the line search.","category":"page"},{"location":"60-speed-up/#R2-(1st-order)","page":"Speed Up","title":"R2 (1st order)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"η1 = eps(T)^(1/4), η2 = T(0.95): step acceptance parameters;\nγ1 = T(1/2), γ2 = 1/γ1: regularization update parameters;\nσmin = eps(T): step parameter for R2 algorithm;\nβ = T(0) ∈ [0,1] is the constant in the momentum term. If β == 0, R2 does not use momentum.","category":"page"},{"location":"60-speed-up/#TRUNK-(matrix-free)","page":"Speed Up","title":"TRUNK (matrix-free)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"bk_max::Int = 10: algorithm parameter;\nmonotone::Bool = true: algorithm parameter;\nnm_itmax::Int = 25: algorithm parameter.","category":"page"},{"location":"60-speed-up/#Bound-constrained-(matrix-free)","page":"Speed Up","title":"Bound-constrained (matrix-free)","text":"","category":"section"},{"location":"60-speed-up/#TRON","page":"Speed Up","title":"TRON","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"μ₀::T = T(1e-2): algorithm parameter in (0, 0.5);\nμ₁::T = one(T): algorithm parameter in (0, +∞);\nσ::T = T(10): algorithm parameter in (1, +∞);\nmax_cgiter::Int = 50: subproblem's iteration limit;\ncgtol::T = T(0.1): subproblem tolerance.","category":"page"},{"location":"60-speed-up/#Constrained","page":"Speed Up","title":"Constrained","text":"","category":"section"},{"location":"60-speed-up/#RipQP-(quadratic-with-linear-constraints)","page":"Speed Up","title":"RipQP (quadratic with linear constraints)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"See RipQP.jl tutorial and RipQP.jl API.","category":"page"},{"location":"60-speed-up/#CaNNOLeS-(NLS-with-nonlinear-equality-constraints)","page":"Speed Up","title":"CaNNOLeS (NLS with nonlinear equality constraints)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"linsolve::Symbol = :ma57: solver to compute LDLt factorization. Available methods are: :ma57, :ldlfactorizations;\nmethod::Symbol = :Newton: available methods :Newton, :LM, :Newton_noFHess, and :Newton_vanishing;","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"See CaNNOLeS.jl tutorial.","category":"page"},{"location":"60-speed-up/#DCISolver-(nonlinear-equality-constraints)","page":"Speed Up","title":"DCISolver (nonlinear equality constraints)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"linear_solver = :ldlfact: Solver for the factorization. options: :ma57 if HSL.jl available.","category":"page"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"See the fine-tuneDCI tutorial.","category":"page"},{"location":"60-speed-up/#FletcherPenaltySolver-(nonlinear-equality-constraints)","page":"Speed Up","title":"FletcherPenaltySolver (nonlinear equality constraints)","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"See the fine-tuneFPS tutorial.","category":"page"},{"location":"60-speed-up/#Percival","page":"Speed Up","title":"Percival","text":"","category":"section"},{"location":"60-speed-up/","page":"Speed Up","title":"Speed Up","text":"μ::Real = T(10.0): Starting value of the penalty parameter.","category":"page"},{"location":"30-qp/#qp","page":"Qp","title":"Quadratic models with linear constraints","text":"","category":"section"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"The quadratic model with linear constraints is another specific case where the objective function is quadratic and the constraints are linear.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"beginaligned\nmin quad   x^TQx + c^Tx + c_0  \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"This problem is convex whenever the matrix Q is positive semi-definite. A key aspect here is the modeling of the matrices Q and A. The main data structure available in Julia are: LinearAlgebra.Matrix, SparseArrays.sparse, SparseMatricesCOO.sparse, LinearOperators.LinearOperator.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"In JuliaSmoothOptimizers, the package QuadraticModels.jl can be used to access the NLPModel API for such instance.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"The function minimize with the following sets of arguments will automatically build a QuadraticModel and choose the adequate solver.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"  stats = minimize(c, H, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = minimize(c, H, lvar, uvar, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = minimize(c, H, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\n  stats = minimize(c, H, lvar, uvar, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)","category":"page"},{"location":"30-qp/#Examples","page":"Qp","title":"Examples","text":"","category":"section"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"  using SparseArrays\n  n = 50\n  c = zeros(n)\n  H = spdiagm(0 => 1.0:n)\n  H[n, 1] = 1.0\n  A = ones(1, n)\n  lcon = [1.0]\n  ucon = [1.0]","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"The quadratic model can then be solved using minimize.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"using JSOSuite\n  stats = minimize(c, H, A, lcon, ucon, name = \"eqconqp_QP\")","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"This is equivalent to building a QuadraticModel and then minimize.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"using QuadraticModels, JSOSuite\n  qp_model = QuadraticModel(c, H, A, lcon, ucon, name = \"eqconqp_QP\")\n  stats = minimize(qp_model)","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"As usual, it is also possible to select manually the solver to be used.","category":"page"},{"location":"30-qp/","page":"Qp","title":"Qp","text":"  using RipQP\n  stats = minimize(\"RipQP\", c, H, A, lcon, ucon, name = \"eqconqp_QP\")","category":"page"},{"location":"10-tutorial/#tutorial-section","page":"Tutorial","title":"Tutorial","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"In this tutorial, we provide examples of usage of the minimize function exported by JSOSuite.jl.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"There are two important challenges in solving an optimization problem: (i) model the problem, and (ii) solve the problem with an appropriate optimizer.","category":"page"},{"location":"10-tutorial/#Modeling","page":"Tutorial","title":"Modeling","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"All these optimizers rely on the NLPModel API from NLPModels.jl for general nonlinear optimization problems of the form","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"The function minimize accepts as an argument any model nlp subtype of AbstractNLPModel.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"output = minimize(nlpmodel::AbstractNLPModel; kwargs...)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"In the rest of this section, we focus on examples using generic modeling tools.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"It is generally of great interest if available to use a modeling that exploits the structure of the problem, see Nonlinear Least Squares for an example with nonlinear least squares.","category":"page"},{"location":"10-tutorial/#JuMP-Model","page":"Tutorial","title":"JuMP Model","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JuMP, JSOSuite\nmodel = Model()\n@variable(model, x)\n@variable(model, y)\n@NLobjective(model, Min, (1 - x)^2 + 100 * (y - x^2)^2)\n\nminimize(model)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"We refer to JuMP tutorial for more on modeling problems with JuMP.","category":"page"},{"location":"10-tutorial/#NLPModel-with-Automatic-Differentiation","page":"Tutorial","title":"NLPModel with Automatic Differentiation","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"We refer to ADNLPModel for the description of the different constructors.","category":"page"},{"location":"10-tutorial/#Unconstrained","page":"Tutorial","title":"Unconstrained","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nstats = minimize(f, x0, verbose = 0)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = minimize(nlp)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"One of the main advantages of this constructor is the possibility to run computations in different arithmetics.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = Float32[-1.2; 1.0]\nstats = minimize(f, x0, verbose = 0)","category":"page"},{"location":"10-tutorial/#Bound-constrained","page":"Tutorial","title":"Bound-constrained","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nlvar, uvar = 2 * ones(2), 4 * ones(2)\nstats = minimize(f, x0, lvar, uvar, verbose = 0)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = minimize(nlp)","category":"page"},{"location":"10-tutorial/#Nonlinear-constrained","page":"Tutorial","title":"Nonlinear constrained","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nc = x -> [x[1]]\nl = ones(1)\nstats = minimize(f, x0, c, l, l, verbose = 0)","category":"page"},{"location":"10-tutorial/#Linearly-constrained","page":"Tutorial","title":"Linearly constrained","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite, SparseArrays\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nA = sparse([\n    1.0 0.0;\n    2.0 3.0\n])\nl = ones(2)\nstats = minimize(f, x0, A, l, l, verbose = 0)","category":"page"},{"location":"10-tutorial/#All-constraints","page":"Tutorial","title":"All constraints","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite, SparseArrays\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nA = sparse([2.0 3.0])\nc = x -> [x[1]]\nl = ones(2)\nstats = minimize(f, x0, A, c, l, l, verbose = 0)","category":"page"},{"location":"10-tutorial/#Optimizing","page":"Tutorial","title":"Optimizing","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"Internally, the minimize function selects optimizers according to the problem's property and their availability.","category":"page"},{"location":"10-tutorial/#Available-optimizers","page":"Tutorial","title":"Available optimizers","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"All the information used by the handled optimizers is available in the following DataFrame:","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using JSOSuite\nJSOSuite.optimizers","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"Required information can be extracted by simple DataFrame manipulations. For instance, the list of optimizers handled by this package","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"JSOSuite.optimizers.name","category":"page"},{"location":"10-tutorial/#Select-optimizers","page":"Tutorial","title":"Select optimizers","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"The function JSOSuite.select_optimizers returns a list of compatible optimizers.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nJSOSuite.select_optimizers(nlp)","category":"page"},{"location":"10-tutorial/#Fine-tune-solve-call","page":"Tutorial","title":"Fine-tune solve call","text":"","category":"section"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"All the keyword arguments are passed to the solver. Keywords available for all the solvers are given below:","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"atol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nmax_time::Float64 = 300.0: maximum number of seconds;\nmax_iter::Int = typemax(Int): maximum number of iterations;\nmax_eval::Int = 10 000: maximum number of constraint and objective functions evaluations;\ncallback = (args...) -> nothing: callback called at each iteration;\nverbose::Int = 0: if > 0, display iteration details for every verbose iteration.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = minimize(nlp, atol = 1e-5, rtol = 1e-7, max_time = 10.0, max_eval = 10000, verbose = 1)","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"Further possible options are documented in each solver's documentation. For instance, we can update the mem parameter of LBFGS.","category":"page"},{"location":"10-tutorial/","page":"Tutorial","title":"Tutorial","text":"using ADNLPModels, JSOSuite\nf = x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats = minimize(\"LBFGS\", nlp, mem = 10, verbose = 1)","category":"page"},{"location":"40-resolve/#resolve","page":"Resolve","title":"Re-solve and in-place solve","text":"","category":"section"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"It is very convenient to pre-allocate the memory used during the optimization of a given problem either for improved memory management or re-solving the same or a similar problem.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Let us consider the following 2-dimensional unconstrained problem","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"beginaligned\nmin_x quad  f(x)= x_2^2 exp(x_1^2)\nendaligned","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Using JSOSuite’s minimize function, the problem can be solved as follows","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"using JSOSuite\nf(x) = x[2]^2 * exp(x[1]^2)\nstats = minimize(f, ones(2))\n;","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Using L-BFGS, the problem is locally solved.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Note that when passing Julia functions as input to minimize, the problem is modeled as an ADNLPModel. So, the following would be equivalent:","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"using ADNLPModels, JSOSuite\nf(x) = x[2]^2 * exp(x[1]^2)\nnlp = ADNLPModel(f, ones(2))\nstats = minimize(nlp)","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"The procedure is similar with JuMP models.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"using JuMP, JSOSuite\nmodel = Model()\n@variable(model, x[i=1:2], start = [1.0; 1.0][i])\n@objective(model, Min, x[2]^2 * exp(x[1]^2))\nstats = minimize(model)","category":"page"},{"location":"40-resolve/#In-place-solve","page":"Resolve","title":"In-place solve","text":"","category":"section"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"If we want to solve the same problem several times, for instance, for several initial guesses, it is recommended to use an in-place solve.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"using ADNLPModels, JSOSolvers, SolverCore\nf(x) = x[2]^2 * exp(x[1]^2)\nnlp = ADNLPModel(f, ones(2)) # or use JuMP\nsolver = JSOSolvers.LBFGSSolver(nlp)\nstats = SolverCore.GenericExecutionStats(nlp)\nsolve!(solver, nlp, stats, x = ones(2))","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"This deserves more explanations. The name of the solver structure and the corresponding package can be accessed via the DataFrame JSOSuite.optimizers.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"JSOSuite.optimizers[!, [:name_solver, :name_pkg]]","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"In our example, the solver L-BFGS is implemented in JSOSolvers.jl and the solver structure is LBFGSSolver.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Now, it is possible to reuse the memory allocated for the first solve for another round:","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"# NLPModels.reset!(nlp) # would also reset the evaluation counters of the model\nSolverCore.reset!(solver)\nnew_x0 = [1.4; 5.0] # another initial guess\nsolve!(solver, nlp, stats, x = new_x0)  # new solve with existing solver object","category":"page"},{"location":"40-resolve/#In-place-solve-of-a-different-problem","page":"Resolve","title":"In-place solve of a different problem","text":"","category":"section"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"It is also possible to reuse the allocated memory to solve another problem with the same number of variables and constraints:","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"f2(x) = x[2]^2 + exp(x[1]^2)\nnlp = ADNLPModel(f2, ones(2)) # or use JuMP\nSolverCore.reset!(solver, nlp)\nsolve!(solver, nlp, stats)","category":"page"},{"location":"40-resolve/#Allocation-free-solvers","page":"Resolve","title":"Allocation-free solvers","text":"","category":"section"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"In order to measure, the amount of memory allocated by the solvers, the package NLPModelsTest.jl defines a set of test problems that are allocation free.","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"using NLPModelsTest, SolverCore, JSOSolvers\nnlp = BROWNDEN(Float64)\nsolver = LBFGSSolver(nlp)\nstats = GenericExecutionStats(nlp)\nsolve!(solver, nlp, stats)\n@allocated solve!(solver, nlp, stats)","category":"page"},{"location":"40-resolve/","page":"Resolve","title":"Resolve","text":"Several of the pure Julia solvers available in JSOSuite have this property.","category":"page"},{"location":"91-developer/#dev_docs","page":"Developer docs","title":"Developer documentation","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"note: Contributing guidelines\nIf you haven't, please read the Contributing guidelines first.","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"If you want to make contributions to this package that involves code, then this guide is for you.","category":"page"},{"location":"91-developer/#First-time-clone","page":"Developer docs","title":"First time clone","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"tip: If you have writing rights\nIf you have writing rights, you don't have to fork. Instead, simply clone and skip ahead. Whenever upstream is mentioned, use origin instead.","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"If this is the first time you work with this repository, follow the instructions below to clone the repository.","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Fork this repo\nClone your repo (this will create a git remote called origin)\nAdd this repo as a remote:\ngit remote add upstream https://github.com/JuliaSmoothOptimizers/JSOSuite.jl","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"This will ensure that you have two remotes in your git: origin and upstream. You will create branches and push to origin, and you will fetch and update your local main branch from upstream.","category":"page"},{"location":"91-developer/#Linting-and-formatting","page":"Developer docs","title":"Linting and formatting","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Install a plugin on your editor to use EditorConfig. This will ensure that your editor is configured with important formatting settings.","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"We use https://pre-commit.com to run the linters and formatters. In particular, the Julia code is formatted using JuliaFormatter.jl, so please install it globally first:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"julia> # Press ]\npkg> activate\npkg> add JuliaFormatter","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"To install pre-commit, we recommend using pipx as follows:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"# Install pipx following the link\npipx install pre-commit","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"With pre-commit installed, activate it as a pre-commit hook:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"pre-commit install","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"To run the linting and formatting manually, enter the command below:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"pre-commit run -a","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Now, you can only commit if all the pre-commit tests pass.","category":"page"},{"location":"91-developer/#Testing","page":"Developer docs","title":"Testing","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"As with most Julia packages, you can just open Julia in the repository folder, activate the environment, and run test:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"julia> # press ]\npkg> activate .\npkg> test","category":"page"},{"location":"91-developer/#Working-on-a-new-issue","page":"Developer docs","title":"Working on a new issue","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"We try to keep a linear history in this repo, so it is important to keep your branches up-to-date.","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Fetch from the remote and fast-forward your local main\ngit fetch upstream\ngit switch main\ngit merge --ff-only upstream/main\nBranch from main to address the issue (see below for naming)\ngit switch -c 42-add-answer-universe\nPush the new local branch to your personal remote repository\ngit push -u origin 42-add-answer-universe\nCreate a pull request to merge your remote branch into the org main.","category":"page"},{"location":"91-developer/#Branch-naming","page":"Developer docs","title":"Branch naming","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"If there is an associated issue, add the issue number.\nIf there is no associated issue, and the changes are small, add a prefix such as \"typo\", \"hotfix\", \"small-refactor\", according to the type of update.\nIf the changes are not small and there is no associated issue, then create the issue first, so we can properly discuss the changes.\nUse dash separated imperative wording related to the issue (e.g., 14-add-tests, 15-fix-model, 16-remove-obsolete-files).","category":"page"},{"location":"91-developer/#Commit-message","page":"Developer docs","title":"Commit message","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Use imperative or present tense, for instance: Add feature or Fix bug.\nHave informative titles.\nWhen necessary, add a body with details.\nIf there are breaking changes, add the information to the commit message.","category":"page"},{"location":"91-developer/#Before-creating-a-pull-request","page":"Developer docs","title":"Before creating a pull request","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"tip: Atomic git commits\nTry to create \"atomic git commits\" (recommended reading: The Utopic Git History).","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Make sure the tests pass.\nMake sure the pre-commit tests pass.\nFetch any main updates from upstream and rebase your branch, if necessary:\ngit fetch upstream\ngit rebase upstream/main BRANCH_NAME\nThen you can open a pull request and work with the reviewer to address any issues.","category":"page"},{"location":"91-developer/#Building-and-viewing-the-documentation-locally","page":"Developer docs","title":"Building and viewing the documentation locally","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Following the latest suggestions, we recommend using LiveServer to build the documentation. Here is how you do it:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Run julia --project=docs to open Julia in the environment of the docs.\nIf this is the first time building the docs\nPress ] to enter pkg mode\nRun pkg> dev . to use the development version of your package\nPress backspace to leave pkg mode\nRun julia> using LiveServer\nRun julia> servedocs()","category":"page"},{"location":"91-developer/#Making-a-new-release","page":"Developer docs","title":"Making a new release","text":"","category":"section"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"To create a new release, you can follow these simple steps:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Create a branch release-x.y.z\nUpdate version in Project.toml\nUpdate the CHANGELOG.md:\nRename the section \"Unreleased\" to \"[x.y.z] - yyyy-mm-dd\" (i.e., version under brackets, dash, and date in ISO format)\nAdd a new section on top of it named \"Unreleased\"\nAdd a new link in the bottom for version \"x.y.z\"\nChange the \"[unreleased]\" link to use the latest version - end of line, vx.y.z ... HEAD.\nCreate a commit \"Release vx.y.z\", push, create a PR, wait for it to pass, merge the PR.\nGo back to main screen and click on the latest commit (link: https://github.com/JuliaSmoothOptimizers/JSOSuite.jl/commit/main)\nAt the bottom, write @JuliaRegistrator register","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"After that, you only need to wait and verify:","category":"page"},{"location":"91-developer/","page":"Developer docs","title":"Developer docs","text":"Wait for the bot to comment (should take < 1m) with a link to a RP to the registry\nFollow the link and wait for a comment on the auto-merge\nThe comment should said all is well and auto-merge should occur shortly\nAfter the merge happens, TagBot will trigger and create a new GitHub tag. Check on https://github.com/JuliaSmoothOptimizers/JSOSuite.jl/releases\nAfter the release is create, a \"docs\" GitHub action will start for the tag.\nAfter it passes, a deploy action will run.\nAfter that runs, the stable docs should be updated. Check them and look for the version number.","category":"page"},{"location":"95-reference/#reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"95-reference/#Contents","page":"Reference","title":"Contents","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Pages = [\"95-reference.md\"]","category":"page"},{"location":"95-reference/","page":"Reference","title":"Reference","text":"Modules = [JSOSuite]","category":"page"},{"location":"95-reference/#JSOSuite.optimizers","page":"Reference","title":"JSOSuite.optimizers","text":"optimizers\n\nDataFrame with the JSO-compliant solvers and their properties.\n\nFor each solver, the following are available:\n\nname::String: name of the solver;\nname_solver::Symbol: name of the solver structure for in-place solve, :not_implemented if not implemented;\nname_parameters::Symbol: name of the parameter structure, :not_implemented if not implemented;\nname_pkg::String: name of the package implementing this solver or its NLPModel wrapper;\nsolve_function::Symbol: name of the function;\nis_available::Bool: true if the solver is available;\nbounds::Bool: true if the solver can handle bound constraints;\nequalities::Bool: true if the solver can handle equality constraints;\ninequalities::Bool: true if the solver can handle inequality constraints;\nspecialized_nls::Bool: true if the solver has a specialized variant for nonlinear least squares;\ncan_solve_nlp::Bool: true if the solver can solve general problems. Some may only solve nonlinear least squares;\nnonlinear_obj::Bool: true if the solver can handle nonlinear objective;\nnonlinear_con::Bool: true if the solver can handle nonlinear constraints;\ndouble_precision_only::Bool: true if the solver only handles double precision (Float64);\nhighest_derivative::Int: order of the highest derivative used by the algorithm.\n\n\n\n\n\n","category":"constant"},{"location":"95-reference/#JSOSuite.bmark_solvers","page":"Reference","title":"JSOSuite.bmark_solvers","text":"bmark_solvers(problems, solver_names::Vector{String}; kwargs...)\nbmark_solvers(problems, solver_names::Vector{String}, solvers::Dict{Symbol, Function}; kwargs...)\n\nWrapper to the function SolverBenchmark.bmark_solvers.\n\nArguments\n\nproblems: The set of problems to pass to the solver, as an iterable ofAbstractNLPModel;\nsolver_names::Vector{String}: The names of the benchmarked solvers. They should be valid JSOSuite names, see JSOSuite.solvers.name for a list;\nsolvers::solvers::Dict{Symbol, Function}: A dictionary of additional solvers to benchmark.\n\nOutput\n\nA Dict{Symbol, DataFrame} of statistics.\n\nKeyword Arguments\n\nThe following keywords available are passed to the JSOSuite solvers:\n\natol: absolute tolerance;\nrtol: relative tolerance;\nmax_time: maximum number of seconds;\nmax_eval: maximum number of cons + obj evaluations;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nAll the remaining keyword arguments are passed to the function SolverBenchmark.bmark_solvers.\n\nExamples\n\nusing ADNLPModels, JSOSuite, Logging, SolverBenchmark\nnlps = (\n  ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n  ADNLPModel(x -> 4 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n)\nnames = [\"LBFGS\", \"TRON\"] # see `JSOSuite.optimizers.name` for the complete list\nstats = with_logger(NullLogger()) do\n  bmark_solvers(nlps, names, atol = 1e-3, colstats = [:name, :nvar, :ncon, :status])\nend\nkeys(stats)\n\nThe second example shows how to add you own solver to the benchmark.\n\nusing ADNLPModels, JSOSolvers, JSOSuite, Logging, SolverBenchmark\nnlps = (\n  ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n  ADNLPModel(x -> 4 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0]),\n)\nnames = [\"LBFGS\", \"TRON\"] # see `JSOSuite.optimizers.name` for the complete list\nother_solvers = Dict{Symbol, Function}(\n  :test => nlp -> lbfgs(nlp; mem = 2, atol = 1e-3, verbose = 0),\n)\nstats = with_logger(NullLogger()) do\n  bmark_solvers(nlps, names, other_solvers, atol = 1e-3, colstats = [:name, :nvar, :ncon, :status])\nend\nkeys(stats)\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#JSOSuite.feasible_point","page":"Reference","title":"JSOSuite.feasible_point","text":"stats = feasible_point(nlp::Union{AbstractNLPModel, JuMP.Model}; kwargs...)\nstats = feasible_point(nlp::Union{AbstractNLPModel, JuMP.Model}, solver_name::Symbol; kwargs...)\n\nCompute a feasible point of the optimization problem nlp. The signature is the same as the function minimize.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl, where the status, solution, primal_residual, iter and time are filled-in.\n\nusing ADNLPModels, JSOSuite\nc(x) = [10 * (x[2] - x[1]^2); x[1] - 1]\nb = zeros(2)\nnlp = ADNLPModel(x -> 0.0, [-1.2; 1.0], c, b, b)\nstats = feasible_point(nlp, verbose = 0)\nstats\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#JSOSuite.generic","page":"Reference","title":"JSOSuite.generic","text":"Checker whether optimizers are Generic only\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#JSOSuite.isnls-Tuple{Any}","page":"Reference","title":"JSOSuite.isnls","text":"isnls(nlp::ADNLPModels.AbstractADNLPModel)\nisnls(nlp::MathOptInterface.Nonlinear.Model)\nisnls(nlp::JuMP.Model)\nisnls(nlp::MathOptNLPModel)\n\nCheck if the given model has a nonlinear least squares objective.\n\nThe function uses the package ExpressionTreeForge to get the expression tree of the objective function, then try to detect the least squares pattern. There is no guarantee that this function detects it accurately.\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#JSOSuite.minimize","page":"Reference","title":"JSOSuite.minimize","text":"stats = minimize(nlp::Union{AbstractNLPModel, JuMP.Model}; kwargs...)\n\nCompute a local minimum of the optimization problem nlp.\n\nstats = minimize(f::Function, x0::AbstractVector, args...; kwargs...)\nstats = minimize(F::Function, x0::AbstractVector, nequ::Integer, args...; kwargs...)\n\nDefine an NLPModel using ADNLPModel.\n\nstats = minimize(c, H, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = minimize(c, H, lvar, uvar, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = minimize(c, H, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\nstats = minimize(c, H, lvar, uvar, A, lcon, ucon, c0 = c0, x0 = x0, name = name; kwargs...)\n\nDefine a QuadraticModel using QuadraticModel.\n\nThe optimizer can be chosen as follows.\n\nstats = minimize(optimizer_name::String, args...; kwargs...)\n\nJuMP.Model are converted in NLPModels via NLPModelsJuMP.jl.\n\nIf your optimization problem has a quadratic or linear objective and linear constraints consider using QuadraticModels.jl or LLSModels.jl for the model definition.\n\nKeyword Arguments\n\nAll the keyword arguments are passed to the selected solver. Keywords available for all the solvers are given below:\n\natol: absolute tolerance;\nrtol: relative tolerance;\nmax_time: maximum number of seconds;\nmax_iter: maximum number of iterations;\nmax_eval: maximum number of cons + obj evaluations;\ncallback = (args...) -> nothing: callback called at each iteration;\nverbose::Int = 0: if > 0, display iteration details every verbose iteration.\n\nThe following are specific to nonlinear least squares:\n\nFatol::T = √eps(T): absolute tolerance on the residual;\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.\n\nFurther possible options are documented in each solver's documentation.\n\nCallback\n\nThe callback is called at each iteration. The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver.\n\nOutput\n\nThe value returned is a GenericExecutionStats, see SolverCore.jl.\n\nExamples\n\nusing JSOSuite\nstats = minimize(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0], verbose = 0)\nstats\n\nThe list of available solver can be obtained using JSOSuite.optimizers[!, :name] or see select_optimizers.\n\nusing JSOSuite\nstats = minimize(\"TRON\", x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0], verbose = 0)\nstats\n\nSome optimizers are available after loading only.\n\nusing JSOSuite\n# We minimize here a quadratic problem with bound-constraints\nc = [1.0; 1.0]\nH = [-2.0 0.0; 3.0 4.0]\nuvar = [1.0; 1.0]\nlvar = [0.0; 0.0]\nx0 = [0.5; 0.5]\nstats = minimize(\"TRON\", c, H, lvar, uvar, x0 = x0, name = \"bndqp_QP\", verbose = 0)\nstats\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#JSOSuite.multi_start","page":"Reference","title":"JSOSuite.multi_start","text":"multi_start(nlp::AbstractNLPModel; kwargs...)\n\nThis function runs a simple optimization strategy that run local optimizers  for several initial guesses.\n\nArguments\n\nnlp::AbstractNLPModel{T, V} represents the model to solve, see NLPModels.jl.\n\nThe keyword arguments may include\n\nmulti_solvers::Bool = false: If true it runs all the available solvers, one solver otherwise;\nskip_solvers::Vector{String} = String[]: If multi_solvers is true, the solvers in this list are skipped;\nN::Integer = get_nvar(nlp): number of additional initial guesses considered;\nmax_time::Float64 = 60.0: maximum time limit in seconds;\nverbose::Integer = 0: if > 0, display iteration details every verbose iteration.\nsolver_verbose::Integer = 0: verbosity of the solver;\nstrategy::Symbol = :random: strategy to compute a next initial guess in [:random].\n\nOther keyword arguments are passed to the solvers.\n\n\n\n\n\n","category":"function"},{"location":"95-reference/#JSOSuite.select_optimizers-Union{Tuple{NLPModels.AbstractNLPModel{T, S}}, Tuple{S}, Tuple{T}} where {T, S}","page":"Reference","title":"JSOSuite.select_optimizers","text":"select_optimizers(nlp::Union{AbstractNLPModel, JuMP.Model}, verbose = 1, highest_derivative_available::Integer = 2)\n\nNarrow the list of optimizers to solve nlp problem using highest_derivative_available.\n\nThis function checks whether the model has:\n\nlinear or nonlinear constraints;\nunconstrained, bound constraints, equality constraints, inequality constraints;\nnonlinear or quadratic objective.\n\nA linear or quadratic objective is detected if the type of nlp is a QuadraticModel or an LLSModel. The selection between a general optimization problem and a nonlinear least squares is done in minimize.\n\nIf no optimizers were selected, consider setting verbose to true to see what went wrong.\n\nOutput\n\nselected_optimizers::DataFrame: A subset of optimizers adapted to the problem nlp.\n\nSee also minimize.\n\nExamples\n\nusing ADNLPModels, JSOSuite\nnlp = ADNLPModel(x -> 100 * (x[2] - x[1]^2)^2 + (x[1] - 1)^2, [-1.2; 1.0])\nselected_optimizers = JSOSuite.select_optimizers(nlp)\nprint(selected_optimizers[!, :name])\n\nusing ADNLPModels, JSOSuite\nnlp = ADNLSModel(x -> [10 * (x[2] - x[1]^2), (x[1] - 1)], [-1.2; 1.0], 2)\nselected_optimizers = JSOSuite.select_optimizers(nlp)\nprint(selected_optimizers[!, :name])\n\n\n\n\n\n","category":"method"},{"location":"95-reference/#SolverCore.solve!-Tuple{Any, Vararg{Any}}","page":"Reference","title":"SolverCore.solve!","text":"solve!(solver::AbstractOptimizationSolver, model::Union{AbstractNLPModel, JuMP.Model}; kwargs...)\nsolve!(solver::AbstractOptimizationSolver, model::Union{AbstractNLPModel, JuMP.Model}, stats; kwargs...)\n\nJSOSuite extension of SolverCore.solve!. The first argument should be of type SolverCore.AbstractOptimizationSolver, see for instance JSOSuite.optimizers[!, :name_solver].\n\n\n\n\n\n","category":"method"},{"location":"20-nonlinear-least-squares/#nls-section","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The nonlinear least squares (NLS) optimization problem is a specific case where the objective function is a sum of squares.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  f(x)=tfrac12F(x)^2_2 \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Although the problem can be solved using only  f, knowing  F independently allows the development of more efficient methods.","category":"page"},{"location":"20-nonlinear-least-squares/#Model-and-solve-NLS","page":"Nonlinear Least Squares","title":"Model and solve NLS","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"In this tutorial, we consider the following equality-constrained problem","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  f(x)=tfrac12(10 * (x2 - x1^2))^2 + tfrac12(x1 - 1)^2 \n 1 leq x1 * x2 leq 1\nendaligned","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"where 1 leq x1 x2 leq 1 implies that x1 x2 = 1.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"In the rest of this tutorial, we will see two ways to model this problem exploiting the knowledge of the structure of the problem.","category":"page"},{"location":"20-nonlinear-least-squares/#NLS-using-automatic-differentiation","page":"Nonlinear Least Squares","title":"NLS using automatic differentiation","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Using the package ADNLPModels.jl, the problem can be model as an ADNLSModel which will use automatic-differentiation to compute the derivatives.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ADNLPModels, JSOSuite\nF = x -> [10 * (x[2] - x[1]^2); x[1] - 1]\nnres = 2 # size of F(x)\nx0 = [-1.2; 1.0]\nc = x -> [x[1] * x[2]]\nl = [1.]\nnls = ADNLSModel(F, x0, nres, c, l, l, name=\"AD-Rosenbrock\")","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Note that the length of the residual function is given explictly to avoid any superfluous evaluation of this (potentially very large) function.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"stats = minimize(nls)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"JSOSuite.jl uses by default automatic differentiation, so the following code would be equivalent:","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"stats = minimize(F, x0, nres, c, l, l)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"By default, JSOSuite.minimize will use a solver tailored for nonlineat least squares problem. Nevertheless, it is also possible to specify the solver to be used.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using NLPModelsIpopt\nstats = minimize(\"IPOPT\", F, x0, nres, c, l, l)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"We refer to the documentation of ADNLPModels.jl for more details on the AD system use and how to modify it.","category":"page"},{"location":"20-nonlinear-least-squares/#NLS-using-JuMP","page":"Nonlinear Least Squares","title":"NLS using JuMP","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The package NLPModelsJuMP.jl exports a constructor, MathOptNLSModel, to build an AbstractNLSModel using JuMP.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using JuMP, JSOSuite, NLPModelsJuMP\n\nmodel = Model()\nx0 = [-1.2; 1.0]\n@variable(model, x[i=1:2], start=x0[i])\n@NLexpression(model, F1, x[1] - 1)\n@NLexpression(model, F2, 10 * (x[2] - x[1]^2))\n@NLconstraint(model, x[1] * x[2] == 1)\n\nnls = MathOptNLSModel(model, [F1, F2], name=\"Ju-Rosenbrock\")\nstats = minimize(nls)","category":"page"},{"location":"20-nonlinear-least-squares/#JSOSuite-automatic-detection","page":"Nonlinear Least Squares","title":"JSOSuite automatic detection","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The package can be used to try detecting NLS-pattern in a model.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ADNLPModels, JSOSuite\nf(x) = (x[2] - x[1]^3)^2 + (x[1] - 1)^2\nx0 = [-1.2; 1.0]\nnlp = ADNLPModel(f, x0)\nstats_nlp = minimize(nlp)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"The function isnls requires the package ExpressionTreeForge.jl. In this example, it detects that the objective function is a nonlinear least squares.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ExpressionTreeForge\nJSOSuite.isnls(nlp)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Therefore, defining an ADNLSModel might improve the solver's behavior.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"F(x) = [x[2] - x[1]^3, x[1] - 1]\nx0 = [-1.2; 1.0]\nnls = ADNLSModel(F, x0, 2)\nstats_nls = minimize(nls)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using NLPModels\n(neval_obj(nlp), neval_obj(nls))","category":"page"},{"location":"20-nonlinear-least-squares/#Find-a-feasible-point-of-an-optimization-problem-or-solve-a-nonlinear-system","page":"Nonlinear Least Squares","title":"Find a feasible point of an optimization problem or solve a nonlinear system","text":"","category":"section"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"We show here how to find the feasible point of a given model.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"beginaligned\nmin quad  tfrac12s^2_2 \n 0 leq s - c(x) leq 0\n ell leq x leq u\nendaligned","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"This formulation can also be used to solve a set of nonlinear equations. Finding a feasible point of an optimization problem is useful to determine whether the problem is feasible or not. Moreover, it is a good practice to find an initial guess.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using ADNLPModels, JSOSuite\n\nf = x -> sum(x.^2)\nx0 = ones(3)\nc = x -> [x[1]]\nb = zeros(1)\nnlp = ADNLPModel(f, x0, c, b, b)\nstats = feasible_point(nlp)","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"Using the function cons from the NLPModel API, we can verify that the obtained solution is feasible.","category":"page"},{"location":"20-nonlinear-least-squares/","page":"Nonlinear Least Squares","title":"Nonlinear Least Squares","text":"using NLPModels\n\ncons(nlp, stats.solution) # is close to zero.","category":"page"},{"location":"90-contributing/#contributing","page":"Contributing","title":"Contributing guidelines","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"First of all, thanks for the interest!","category":"page"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"We welcome all kinds of contribution, including, but not limited to code, documentation, examples, configuration, issue creating, etc.","category":"page"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"Be polite and respectful, and follow the code of conduct.","category":"page"},{"location":"90-contributing/#Bug-reports-and-discussions","page":"Contributing","title":"Bug reports and discussions","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"If you think you found a bug, feel free to open an issue. Focused suggestions and requests can also be opened as issues. Before opening a pull request, start an issue or a discussion on the topic, please.","category":"page"},{"location":"90-contributing/#Working-on-an-issue","page":"Contributing","title":"Working on an issue","text":"","category":"section"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"If you found an issue that interests you, comment on that issue what your plans are. If the solution to the issue is clear, you can immediately create a pull request (see below). Otherwise, say what your proposed solution is and wait for a discussion around it.","category":"page"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"tip: Tip\nFeel free to ping us after a few days if there are no responses.","category":"page"},{"location":"90-contributing/","page":"Contributing","title":"Contributing","text":"If your solution involves code (or something that requires running the package locally), check the developer documentation. Otherwise, you can use the GitHub interface directly to create your pull request.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = JSOSuite","category":"page"},{"location":"#JSOSuite","page":"Home","title":"JSOSuite","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JSOSuite is a unique solution to access all the solvers available in the JuliaSmoothOptimizers organization.","category":"page"},{"location":"","page":"Home","title":"Home","text":"All these solvers rely on the NLPModel API from NLPModels.jl for general nonlinear optimization problems of the form","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nmin quad  f(x) \n c_L leq c(x) leq c_U \n c_A leq Ax leq l_A \n ell leq x leq u\nendaligned","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"julia> # Press ]\npkg> add JSOSuite","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package JSOSuite exports a function minimize:","category":"page"},{"location":"","page":"Home","title":"Home","text":"output = minimize(args...; kwargs...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"where the arguments define the problem, see Tutorial.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is also possible to define an NLPModel or a JuMP model representing the problem, and then call minimize:","category":"page"},{"location":"","page":"Home","title":"Home","text":"output = minimize(nlpmodel; kwargs...)\noutput = minimize(jump; kwargs...)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The NLPModel API is a general API for solvers to interact with models by providing flexible data types to represent the objective and constraint functions to evaluate their derivatives, and to provide essentially any information that a solver might request from a model. JuliaSmoothOrganization's website jso.dev or NLPModels.jl's documentation provide more tutorials on this topic.","category":"page"},{"location":"#NLPModel","page":"Home","title":"NLPModel","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"JuliaSmoothOptimizers' compliant solvers accept any model compatible with the NLPModel API. See the Tutorial section for examples.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Depending on the origin of the problem several modeling tools are available. The following generic modeling tools are accepted:","category":"page"},{"location":"","page":"Home","title":"Home","text":"JuMP models are internally made compatible with NLPModel via NLPModelsJuMP.jl;\nAmpl models stored in a .nl file can be instantiated with AmplModel(\"name_of_file.nl\") using AmplNLReader.jl;\nQPSReader.jl reads linear problems in MPS format and quadratic problems in QPS format;\nModels using automatic differentiation can be generated using ADNLPModels.jl;\nModels with manually input derivatives can be defined using ManualNLPModels.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It is also possible to define your NLPModel variant. Several examples are available within JuliaSmoothOptimizers' umbrella:","category":"page"},{"location":"","page":"Home","title":"Home","text":"KnetNLPModels.jl: An NLPModels Interface to Knet.\nPDENLPModels.jl: A NLPModel API for optimization problems with PDE-constraints.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A nonlinear least squares problem is a special case with the objective function defined as  f(x) = tfrac12F(x)^2_2. Although the problem can be solved using only  f, knowing  F independently allows the development of more efficient methods. See the Nonlinear Least Squares for more on the special treatment of these problems.","category":"page"},{"location":"#Output","page":"Home","title":"Output","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The value returned is a GenericExecutionStats, which is a structure containing the available information at the end of the execution, such as a solver status, the objective function value, the norm of the residuals, the elapsed time, etc.","category":"page"},{"location":"","page":"Home","title":"Home","text":"It contains the following fields:","category":"page"},{"location":"","page":"Home","title":"Home","text":"status: Indicates the output of the solver. Use show_statuses() for the full list;\nsolution: The final approximation returned by the solver (default: an uninitialized vector like nlp.meta.x0);\nobjective: The objective value at solution (default: Inf);\ndual_feas: The dual feasibility norm at solution (default: Inf);\nprimal_feas: The primal feasibility norm at solution (default: 0.0 if unconstrained, Inf otherwise);\nmultipliers: The Lagrange multipliers wrt to the constraints (default: an uninitialized vector like nlp.meta.y0);\nmultipliers_L: The Lagrange multipliers wrt to the lower bounds on the variables (default: an uninitialized vector like nlp.meta.x0 if there are bounds, or a zero-length vector if not);\nmultipliers_U: The Lagrange multipliers wrt to the upper bounds on the variables (default: an uninitialized vector like nlp.meta.x0 if there are bounds, or a zero-length vector if not);\niter: The number of iterations computed by the solver (default: -1);\nelapsed_time: The elapsed time computed by the solver (default: Inf);\nsolver_specific::Dict{Symbol,Any}: A solver specific dictionary.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The list of statuses is available via the function SolverCore.show_statuses:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using SolverCore\nshow_statuses()","category":"page"},{"location":"#Keyword-Arguments","page":"Home","title":"Keyword Arguments","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"All the keyword arguments are passed to the selected solver. Keywords available for all the solvers are given below:","category":"page"},{"location":"","page":"Home","title":"Home","text":"atol::T = √eps(T): absolute tolerance;\nrtol::T = √eps(T): relative tolerance;\nmax_time::Float64 = 300.0: maximum number of seconds;\nmax_iter::Int = typemax(Int): maximum number of iterations;\nmax_eval::Int = 10 000: maximum number of constraint and objective functions evaluations;\ncallback = (args...) -> nothing: callback called at each iteration;\nverbose::Int = 0: if > 0, display iteration details for every verbose iteration.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The expected signature of the callback is callback(nlp, solver, stats), and its output is ignored. Changing any of the input arguments will affect the subsequent iterations. In particular, setting stats.status = :user will stop the algorithm. All relevant information should be available in nlp and solver.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following are specific to nonlinear least squares:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fatol::T = √eps(T): absolute tolerance on the residual;\nFrtol::T = eps(T): relative tolerance on the residual, the algorithm stops when ‖F(xᵏ)‖ ≤ Fatol + Frtol * ‖F(x⁰)‖.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Further possible options are documented in each solver's documentation.","category":"page"},{"location":"#Contributors","page":"Home","title":"Contributors","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"<!-- ALL-CONTRIBUTORS-LIST:START - Do not remove or modify this section -->\n<!-- prettier-ignore-start -->\n<!-- markdownlint-disable -->\n\n<!-- markdownlint-restore -->\n<!-- prettier-ignore-end -->\n\n<!-- ALL-CONTRIBUTORS-LIST:END -->","category":"page"},{"location":"50-benchmark/#Benchmarking-optimizers","page":"Benchmark","title":"Benchmarking optimizers","text":"","category":"section"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"Benchmarking is very important when researching new algorithms or selecting the most approriate ones.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"The package SolverBenchmark exports the function bmark_solvers that runs a set of optimizers on a set of problems. JSOSuite.jl specialize this function, see bmark_solvers.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"The JuliaSmoothOptimizers organization contains several packages of test problems ready to use for benchmarking. The main ones are","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"OptimizationProblems.jl: This package provides a collection of optimization problems in JuMP and ADNLPModels syntax;\nCUTEst.jl;\nNLSProblems.jl.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"In this tutorial, we will use OptimizationProblems.jl with automatic differentiation.","category":"page"},{"location":"50-benchmark/#Benchmark-with-OptimizationProblems.jl","page":"Benchmark","title":"Benchmark with OptimizationProblems.jl","text":"","category":"section"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"using ADNLPModels, OptimizationProblems, JSOSuite","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"The package OptimizationProblems.jl provides a meta containing all the problem information. It is possible to then select the problems without evaluating them first.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"selected_meta = OptimizationProblems.meta\nselected_meta = selected_meta[(selected_meta.nvar .< 200), :] # choose problem with <200 variables.\nselected_meta = selected_meta[.!selected_meta.has_bounds .&& (selected_meta.ncon .== 0), :]; # unconstrained problems\nlist = selected_meta[!, :name]","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"Then, we generate the list of problems using ADNLPModel.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"ad_problems = [\n  OptimizationProblems.ADNLPProblems.eval(Meta.parse(problem))() for problem ∈ list\n]\nlength(ad_problems) # return the number of problems","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"We now want to select appropriate optimizers using the JSOSuite.optimizers.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"using NLPModelsIpopt\nselected_optimizers = JSOSuite.optimizers\n# optimizers can solve general `nlp` as some are specific to variants (NLS, ...)\nselected_optimizers = selected_optimizers[selected_optimizers.can_solve_nlp, :]\nselected_optimizers[selected_optimizers.is_available, :] # optimizers available","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"For the purpose of this example, we will consider 3 optimizers.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"select = [\"IPOPT\", \"TRUNK\", \"LBFGS\"]","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"Once the problems and optimizers are chosen, the function bmark_solvers runs the benchmark.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"using SolverBenchmark\nstats = bmark_solvers(ad_problems, select, atol = 1e-3, max_time = 10.0, verbose = 0)","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"Finally, the result can be processed. For instance, we show here performance profiles comparing the elapsed time and the number of evaluations.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"first_order(df) = df.status .== :first_order\nunbounded(df) = df.status .== :unbounded\nsolved(df) = first_order(df) .| unbounded(df)\ncostnames = [\"time\", \"obj + grad + hess\"]\ncosts = [\n  df -> .!solved(df) .* Inf .+ df.elapsed_time,\n  df -> .!solved(df) .* Inf .+ df.neval_obj .+ df.neval_grad,\n]\n\nusing Plots, SolverBenchmark\ngr()\n\nprofile_solvers(stats, costs, costnames)","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"Note that there are fundamental differences between these optimizers as highlighted in the following.","category":"page"},{"location":"50-benchmark/","page":"Benchmark","title":"Benchmark","text":"for solver in [\"IPOPT\", \"TRUNK\", \"LBFGS\"]\n  println(\"$solver evaluations:\")\n  a, b, c, d = eachcol(\n    stats[Symbol(solver)][!, [:neval_obj, :neval_grad, :neval_hess, :neval_hprod]]\n  )\n  println(\n    \"neval_obj: $(sum(a)),  neval_grad: $(sum(b)), neval_hess: $(sum(c)), neval_hprod: $(sum(d)).\"\n  )\nend","category":"page"}]
}
